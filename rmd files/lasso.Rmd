---
title: "LASSO Results"
author: "Beilin Jia"
date: "April 24, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### LASSO
We first use LASSO to reduce dimensions of document-term matrix (DTM). 

```{r, eval = FALSE}
set.seed(111)
feature.selection <- cv.glmnet(train.dtm, train.sample$toxic, family = "binomial")
# extract parameter estimates based best tuning parameters
beta <- coef(feature.selection,lambda = feature.selection$lambda.1se)
# prepare dense matrix to train random forest and SVM
s.train <- train.dtm[,which(beta!=0)]
dense.train.dtm <- as.matrix(s.train) # selected 122 keywords
```
```{r}
load("./feature_selection.Rdata")
# fit LASSO by using lambda.1se
fit <- glmnet(train.dtm, train.sample$toxic, family = "binomial",
             alpha=1,lambda=feature.selection$lambda.1se)
# LASSO prediction
pred_LASSO <- predict(fit,test.dtm, type = "class")
# prediction accuracy
sum(pred_LASSO==as.vector(test_labels_use$toxic))/length(pred_LASSO)# 0.5484229
```
We also try to use LASSO fit and directly predict on test dataset. But the accuracy is only $54.8\%$, which may not be good enough. So we will explore other machine learning techniques to improve the prediction results. 

#### LASSO --> Random Forest
LASSO helps to reduce the dimension of DTM to $122$, and then we use the `train` function in `caret` to handle the running of models over a grid of parameters. We train both random forest and SVM models and use 10-fold cross validation resampling method for tuning.

Let's first see how we train random forest models. 
```{r, eval=FALSE}
control <- trainControl(method="cv", number=10, search="grid")
metric <- "Accuracy"
set.seed(1)
p <- dim(dense.train.dtm)[2]
tunegrid <- expand.grid(.mtry=c(2^(0:6),p))
# train rf
rf_gridsearch <- train(dense.train.dtm, train.sample$toxic, 
                       method="rf", metric=metric, tuneGrid=tunegrid, 
                       trControl=control)
```
```{r}
# demonstrate cross validation results, create a plot of Kappa against mtry
load("/Users/apple/BIOS735/UmbrellaAcademy/rf_gridsearch2.Rdata")
rf <- rf_gridsearch$results
tune.best <- rf[order(rf$Kappa, decreasing = T)[1],]
library(ggrepel)
library(scales)
ggplot(rf, aes(mtry, Kappa)) + geom_point() + geom_line() + 
  geom_point(data = tune.best, col="black", size=3.5, stroke = 0.8, shape=21) + 
  scale_x_continuous(breaks = c(1,2,4,8,16,32,64)) +
  geom_label_repel(data = tune.best, label = "mtry = 16", color = 'white', 
                   fill = hue_pal()(5)[5], segment.color="grey", nudge_x = 6, nudge_y = -0.05, size = 4) + ggtitle("Cross Validation Results of Random Forest based on LASSO")
```
From this plot, we can see that Kappa increases fast when mtry changes from 1 to 8 and decreases slightly after reaching its maximum at mtry = 16. So we will use mtry = 16 to predict on test dataset. 

```{r}
pred_lassoRF <- predict(rf_gridsearch,as.matrix(test.dtm))
pred_lassoRF <- ifelse(pred_lassoRF==0,"pred_non_toxic","pred_toxic")
test_toxic <- ifelse(test_labels_use$toxic==0,"expect_non_toxic","expect_toxic")
table_lassoRF <- table(pred_lassoRF,test_toxic)
print(table_lassoRF)
```
```{r}
prop_table_lassoRF <- prop.table(table_lassoRF,2)
print(prop_table_lassoRF)
# prediction accuracy
sum(pred_lassoRF==test_toxic)/length(pred_lassoRF)
```
The diagnoal of above confusion matrix represent specificity ($=64.2\%$) and sensitivity ($=84.1\%$). The overall prediction accuracy of random forest is around $66.1\%$. 

#### LASSO --> Linear SVM
Next, we train SVM models with linear kernel and radial basis function kernel. Let's start with linear SVM. 
```{r, eval = FALSE}
set.seed(111)
trCtl = trainControl(method = "cv")
tg <- data.frame(C=c(0.1,0.5,1,10))
fit_svmLinear = train(x = dense.train.dtm, 
                      y = as.factor(train.sample$toxic), 
                      method = "svmLinear",
                      tuneGrid = tg,
                      trControl = trCtl)
```
```{r}
# demonstrate cross validation results, create a plot of Kappa against cost
load("/Users/apple/BIOS735/UmbrellaAcademy/sl.Rdata") # need combine SVM results
svm_linear <- fit_svmLinear$results
tune.best <- svm_linear[order(svm_linear$Kappa, decreasing = T)[1],]
library(ggrepel)
library(scales)
ggplot(svm_linear, aes(C, Kappa)) + geom_point() + geom_line() + 
  geom_point(data = tune.best, col="black", size=3.5, stroke = 0.8, shape=21) + 
  scale_x_continuous(breaks = c(0.1,0.5,1,10)) +
  geom_label_repel(data = tune.best, label = "Cost = 0.1", color = 'white', 
                   fill = hue_pal()(5)[5], segment.color="grey", nudge_x = 0.5, size = 4) + ggtitle("Cross Validation Results of Linear SVM based on LASSO")
```
Based on cross validation results, we obtain the best Kappa when cost equals to 0.1. Also, Kappa has a large drop when cost becomes 0.5. We then use linear SVM with cost = 0.1 to predict toxicity/nontoxicity. 

```{r}
pred_lassoSVMlinear = predict(fit_svmLinear, as.matrix(test.dtm))
pred_lassoSVMlinear <- ifelse(pred_lassoSVMlinear==0,"pred_non_toxic","pred_toxic")
table_lassoSVMlinear <- table(pred_lassoSVMlinear,test_toxic)
print(table_lassoSVMlinear)
```
```{r}
prop_table_lassoSVMlinear <- prop.table(table_lassoSVMlinear,2)
print(prop_table_lassoSVMlinear)
# prediction accuracy
sum(pred_lassoSVMlinear==test_toxic)/length(pred_lassoSVMlinear)
```
For linear SVM, the sensitivity is $85.1\%$ and the specificity equals to $63.5\%$. The overall accuracy is around $65.5\%$. 

#### LASSO --> Radial SVM
Then, we consider the SVM model with radial basis function (RBF) kernel. 
```{r, eval=FALSE}
set.seed(111)
trCtl = trainControl(method = "cv") 
tg <- expand.grid(C = seq(10,100,10), sigma = c(0.5,1,1.5,2,3,4,5,7,9))
fit_svmRadial = train(x = dense.train.dtm, 
                      y = as.factor(train.sample$toxic), 
                      method = "svmRadial", 
                      tuneGrid = tg,
                      trControl = trCtl)
```

```{r}
load("./rsvm.RData") ### need combine rsvm data
rsvm = fit_svmRadial$results[, 1:4]
rsvm = rsvm[rsvm$C %in% seq(10,90,20),]
tune.best = rsvm[order(rsvm$Kappa, decreasing = T)[1],]
library(ggrepel)
# Kappa plot
ggplot(rsvm, aes(sigma, Kappa, col = factor(C))) + geom_point() + geom_line() + 
  geom_point(data = tune.best, col="black", size=3.5, stroke = 0.8, shape=21) + 
  labs(color = "Cost") + scale_x_continuous(breaks = c(0.5,1,1.5,2,3,4,5,7,9)) +
  geom_label_repel(data = tune.best, label = "Cost = 90, sigma = 0.5", color = 'white', 
                   fill = hue_pal()(5)[5], segment.color="black", nudge_x = 3, size = 4) + ggtitle("Cross Validation Results of Radial SVM based on LASSO")
```
Each line on above plot represents a value of cost function we used to tune RBF SVM model. We can see that lines have similar trends as sigma becomes large, which implies that cost function does not have much influence on Kappa. However, Kappa is clearly sensitive to the choices of sigma. The combination of cost = 90 and sigma = 0.5 yields the best value of Kappa. We will then use this combination to obtain prediction results. 

```{r}
pred_lassoSVMRadial = predict(fit_svmRadial, as.matrix(test.dtm))
pred_lassoSVMRadial <- ifelse(pred_lassoSVMRadial==0,"pred_non_toxic","pred_toxic")
table_lassoSVMRadial <- table(pred_lassoSVMRadial,test_toxic)
print(table_lassoSVMRadial)
```
```{r}
prop_table_lassoSVMRadial <- prop.table(table_lassoSVMRadial,2)
print(prop_table_lassoSVMRadial)
# prediction accuracy
sum(pred_lassoSVMRadial==test_toxic)/length(pred_lassoSVMRadial)
```
The sensitivity and specificity are $72.1\%$ and $67.0\%$, respectively. The overall prediction accuracy equals to $67.5\%$. 

Comparing the prediction results obtained from three models based on LASSO, RBF SVM has the highest overall accuracy of $67.5\%$ as well as the best specificity of $67.0\%$, and linear SVM has the best sensitivity of $85.1\%$.

